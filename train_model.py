# ðŸ“Œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
import os
import numpy as np
import pandas as pd
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Dropout, BatchNormalization
from keras.optimizers import AdamW
from keras.losses import LogCosh
from keras.regularizers import l2
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ðŸ“Œ GPU ì‚¬ìš© ì•ˆí•¨ (CPUë¡œ ì‹¤í–‰)
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

# ðŸ“Œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (openpyxl ì—”ì§„ ì¶”ê°€)
file_path = "C:/python_my_projects/homepage2/í•™ìŠµ.xlsx"
df = pd.read_excel(file_path, sheet_name="ì°¨íŠ¸_ì •ë¦¬", engine="openpyxl")

# ðŸ“Œ ìž…ë ¥(X) / ì¶œë ¥(Y) ë°ì´í„° ë¶„ë¦¬
X = df[['ì—°ë©´ì ', 'ì°½ë©´ì ë¹„', 'ì—´ê´€ë¥˜ìœ¨_ì§€ë¶•', 'ì—´ê´€ë¥˜ìœ¨_ë²½ì²´', 'ì—´ê´€ë¥˜ìœ¨_ë°”ë‹¥', 'ì—ë„ˆì§€ ìžë¦½ë¥ ']]
Y = df[['PV ì„¤ì¹˜ ê·œëª¨']]

# ðŸ“Œ ë°ì´í„° ë¶„í•  (í›ˆë ¨ 80%, í…ŒìŠ¤íŠ¸ 20%)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ðŸ“Œ ë°ì´í„° ì •ê·œí™” (í‘œì¤€í™”)
scaler_X = StandardScaler()
scaler_Y = StandardScaler()
X_train = scaler_X.fit_transform(X_train)
X_test = scaler_X.transform(X_test)
Y_train = scaler_Y.fit_transform(Y_train)
Y_test = scaler_Y.transform(Y_test)

# ðŸ“Œ ëžœë¤ ì‹œë“œ ê³ ì • (ê²°ê³¼ ìž¬í˜„ ê°€ëŠ¥)
np.random.seed(42)

# ðŸ“Œ ìµœì í™”ëœ TensorFlow ëª¨ë¸ ìƒì„±
model = Sequential([
    BatchNormalization(input_shape=(X_train.shape[1],)),  # BatchNormì„ ë§¨ ì•žì— ë°°ì¹˜
    Dense(256, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001)), 
    BatchNormalization(),
    Dropout(0.05),  

    Dense(128, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001)),
    BatchNormalization(),
    Dropout(0.05),

    Dense(64, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001)),
    BatchNormalization(),
    Dropout(0.05),

    Dense(1)  # ðŸ”¹ ì¶œë ¥ì¸µ (PV ì„¤ì¹˜ ê·œëª¨ ì˜ˆì¸¡)
])

# ðŸ“Œ AdamW Optimizer ì ìš© (Lookahead ì œê±°)
optimizer = AdamW(learning_rate=0.00002, weight_decay=1e-4)

# ðŸ“Œ ëª¨ë¸ ì»´íŒŒì¼ ë° í•™ìŠµ ì¤€ë¹„
model.compile(optimizer=optimizer, loss=LogCosh())

# ðŸ“Œ Early Stopping ì ìš©
early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)

# ðŸ“Œ ëª¨ë¸ í•™ìŠµ (Epoch=5000 ì„¤ì •)
history = model.fit(
    X_train, Y_train,
    epochs=5000,
    batch_size=512,  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
    validation_data=(X_test, Y_test),
    callbacks=[early_stopping]
)

# ðŸ“Œ ëª¨ë¸ í‰ê°€
loss = model.evaluate(X_test, Y_test)
print(f"\nâœ… ëª¨ë¸ ì†ì‹¤(MSE): {loss}")

# ðŸ“Œ ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡
new_data = np.array([[2000, 0.3, 0.2, 0.7, 0.4, 0.75]])  # ì˜ˆì œ ë°ì´í„°
new_data_scaled = scaler_X.transform(new_data)  # ì •ê·œí™” ì ìš©
predicted_pv = model.predict(new_data_scaled)

# ðŸ“Œ ì˜ˆì¸¡ê°’ ë³µì› (ì •ê·œí™”ëœ ê°’ì„ ì›ëž˜ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜)
predicted_pv = scaler_Y.inverse_transform(predicted_pv)

# ðŸ“Œ ZEB ì¸ì¦ ë“±ê¸‰ ê³„ì‚° (ì—ë„ˆì§€ ìžë¦½ë¥  ê¸°ì¤€)
energy_self_sufficiency = new_data[0][-1]  # ë§ˆì§€ë§‰ ê°’ì´ ì—ë„ˆì§€ ìžë¦½ë¥ 
if energy_self_sufficiency >= 1.0:
    zeb_grade = "ZEB 1ë“±ê¸‰"
elif 0.8 <= energy_self_sufficiency < 1.0:
    zeb_grade = "ZEB 2ë“±ê¸‰"
elif 0.6 <= energy_self_sufficiency < 0.8:
    zeb_grade = "ZEB 3ë“±ê¸‰"
elif 0.4 <= energy_self_sufficiency < 0.6:
    zeb_grade = "ZEB 4ë“±ê¸‰"
elif 0.2 <= energy_self_sufficiency < 0.4:
    zeb_grade = "ZEB 5ë“±ê¸‰"
else:
    zeb_grade = "ZEB ì¸ì¦ ë¶ˆê°€"

# ðŸ“Œ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
print(f"\nðŸ”¹ ì˜ˆì¸¡ëœ PV ì„¤ì¹˜ ê·œëª¨: {predicted_pv[0][0]:.2f} kW")
print(f"ðŸ”¹ ZEB ì¸ì¦ ë“±ê¸‰: {zeb_grade}")

# ðŸ“Œ í•™ìŠµëœ ëª¨ë¸ ì €ìž¥ (h5 í˜•ì‹)
model.save("my_final_best_model.h5")
print("\nâœ… í•™ìŠµëœ ëª¨ë¸ì´ my_final_best_model.h5ë¡œ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
